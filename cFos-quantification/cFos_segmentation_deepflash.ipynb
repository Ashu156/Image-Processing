{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFGqFEE//LvzE1y3ifaaFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashu156/Image-Processing/blob/master/cFos-quantification/cFos_segmentation_deepflash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C8yqFEVhpVQ"
      },
      "outputs": [],
      "source": [
        "!pip install fastcore opencv-python-headless==4.5.4.60\n",
        "!pip install git+https://github.com/MouseLand/cellpose.git@316927eff7ad2201391957909a2114c68baee309"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbDoXA3PKNAm"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tifffile\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from pathlib import Path\n",
        "from cellpose import models, dynamics\n",
        "from fastcore.foundation import L\n",
        "from skimage.color import label2rgb\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNwYiX9RmiMX"
      },
      "source": [
        "# Connect to drive\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "except:\n",
        "  print('Google Drive is not available.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8LFII70JHOl"
      },
      "source": [
        "DATASET = 'cFOS_in_HC'\n",
        "SEED = 0 # We used seeds [0,1,2] in our experiemnts\n",
        "OUTPUT_PATH = Path(\"/content/predictions\") # Save predictions here\n",
        "MODEL_PATH = Path(\"/content/models\") # Save models here\n",
        "PREPROC_PATH = Path(\"/content/data\")\n",
        "DATA_PATH = Path('/gdrive/MyDrive/deepflash2-paper/data')\n",
        "TRAINED_MODEL_PATH= Path('/gdrive/MyDrive/benchmark_models/cellpose/')\n",
        "path = DATA_PATH/DATASET\n",
        "\n",
        "pretrained_dict = {\n",
        "    'PV_in_HC':'cyto', \n",
        "    'cFOS_in_HC':'cyto2',\n",
        "    'mScarlet_in_PAG':'cyto2',\n",
        "    'YFP_in_CTX':'cyto',\n",
        "    'GFAP_in_HC':'cyto2'\n",
        "}\n",
        "\n",
        "diam_dict = {\n",
        "    'PV_in_HC':24, \n",
        "    'cFOS_in_HC':15, \n",
        "    'mScarlet_in_PAG':55, \n",
        "    'YFP_in_CTX':50,\n",
        "    'GFAP_in_HC':17\n",
        "}\n",
        "\n",
        "mean_diam_dict = {\n",
        "    'PV_in_HC':30, \n",
        "    'cFOS_in_HC':17, \n",
        "    'mScarlet_in_PAG':17,\n",
        "    'YFP_in_CTX':30,\n",
        "    'GFAP_in_HC':17,\n",
        "}\n",
        "\n",
        "# cellpose settings\n",
        "channels=[0,0]\n",
        "use_GPU = True\n",
        "device = models.assign_device(True, use_GPU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ugKa2eQhq9"
      },
      "source": [
        "train_image_path = path/'train'/'images'\n",
        "train_masks_path = path/'train'/'masks_STAPLE'\n",
        "\n",
        "train_preproc_path = PREPROC_PATH/DATASET/f'train'\n",
        "train_preproc_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "train_images = [x for x in train_image_path.iterdir() if not x.name.startswith('.')]\n",
        "label_fn = lambda x: train_masks_path/f'{x.name[:-4]}_mask.png'\n",
        "\n",
        "for f in train_images:\n",
        "\n",
        "    # images\n",
        "    img = imageio.imread(f)\n",
        "    shape_orig = img.shape\n",
        "    min_shape = min(img.shape)\n",
        "    img = img[:min_shape,:min_shape]\n",
        "    print(f, shape_orig, img.shape)\n",
        "    tifffile.imwrite(train_preproc_path/f'{f.stem}_img.tif', img, compress=6)\n",
        "\n",
        "    # masks\n",
        "    msk = imageio.imread(label_fn(f))\n",
        "    # These labels can be noisy due to the ground truth estimation procedure (STAPLE)\n",
        "    _, label_msk = cv2.connectedComponents(msk.astype('uint8'), connectivity=4)\n",
        "    tifffile.imwrite(train_preproc_path/f'{f.stem}_masks.tif', label_msk, compress=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj5_-0EaHeU0"
      },
      "source": [
        "test_image_path = path/'test'/'images'\n",
        "test_files = [x for x in test_image_path.iterdir() if not x.name.startswith('.')]\n",
        "test_images = list(tifffile.imread(test_files))\n",
        "    \n",
        "for model_type in ['cellpose_ensemble', 'cellpose_single', 'cellpose']:\n",
        "\n",
        "    if model_type=='cellpose':\n",
        "        print(model_type)\n",
        "        model_type2=pretrained_dict[DATASET]\n",
        "        model = models.Cellpose(gpu=use_GPU, model_type=model_type2)\n",
        "\n",
        "        masks, flows, styles, diams = model.eval(test_images, \n",
        "                                          diameter=diam_dict[DATASET], \n",
        "                                          channels=channels\n",
        "                                        )\n",
        "\n",
        "    else:\n",
        "        print('Using fine tuned model')\n",
        "        pretrained_models = [x.as_posix() for x in (TRAINED_MODEL_PATH/f'{SEED+1}'/DATASET/'models').iterdir() \n",
        "                             if not x.name.endswith('_size.npy') and not x.name.startswith('.')]\n",
        "        if model_type=='cellpose_single': pretrained_models = pretrained_models[:1]\n",
        "        print(pretrained_models)\n",
        "        model = models.CellposeModel(gpu=use_GPU, \n",
        "                                     pretrained_model=pretrained_models,\n",
        "                                     diam_mean=mean_diam_dict[DATASET], \n",
        "                                    )\n",
        "        \n",
        "        masks, flows, styles = model.eval(test_images, \n",
        "                                          diameter=diam_dict[DATASET], \n",
        "                                          channels=channels\n",
        "                                        )\n",
        "\n",
        "\n",
        "    \n",
        "    prediction_path = OUTPUT_PATH/DATASET/model_type\n",
        "    for i, f in enumerate(test_files):\n",
        "        print(f.name)\n",
        "        idx = f.stem\n",
        "        label_msk = masks[i]\n",
        "        # Save semantic segmentation prediction\n",
        "        masks_path = prediction_path/'masks'\n",
        "        masks_path.mkdir(parents=True, exist_ok=True)\n",
        "        imageio.imwrite(masks_path/f'{idx}.png', ((label_msk>0)*255).astype('uint8'))\n",
        "\n",
        "        # Save instance segmentation prediction\n",
        "        instance_masks_path = prediction_path/'instance_masks'\n",
        "        instance_masks_path.mkdir(exist_ok=True)\n",
        "        tifffile.imwrite(instance_masks_path/f'{idx}.tif', label_msk.astype('int16'), compress=6)\n",
        "\n",
        "        # Plot\n",
        "        fig, axs = plt.subplots(ncols=2, figsize=(10,5))\n",
        "        axs[0].imshow(label_msk>0)\n",
        "        axs[0].set_title(f'Semantic segmentation {idx}')\n",
        "        axs[1].imshow(label2rgb(label_msk, bg_label=0))\n",
        "        axs[1].set_title(f'Instance segmentation {idx}')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import io\n",
        "image = io.imread('/gdrive/MyDrive/deepflash2-paper/data/cFOS_in_HC/test/images/MAX_C2-SV7 TM BLA 20X BA 2-2.tif')"
      ],
      "metadata": {
        "id": "MIbuaf43l-hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
        "plt.imshow(image, cmap = 'gray')\n",
        "# plt.imshow('/content/predictions/cFOS_in_HC/cellpose/masks/MAX_C2-SV7 TM BLA 20X BLA 2-2.png')\n",
        "plt.imshow(masks[9], cmap = 'jet', alpha=0.5*(masks[9]>0))"
      ],
      "metadata": {
        "id": "BEiYtxmDkRPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ashu = masks[9]\n",
        "ashu[ashu>0] = 1\n",
        "\n",
        "plt.imshow(ashu, cmap = 'gray')\n",
        "np.max(ashu)"
      ],
      "metadata": {
        "id": "gxz-9yS9LxjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = ndi.distance_transform_edt(masks[9])\n",
        "coords = peak_local_max(distance, footprint=np.ones((30, 30)), labels = masks[9])\n",
        "mask = np.zeros(distance.shape, dtype=bool)\n",
        "mask[tuple(coords.T)] = True\n",
        "markers, _ = ndi.label(mask)\n",
        "labels = watershed(-distance, markers, mask = masks[9])\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
        "ax = axes.ravel()\n",
        "\n",
        "ax[0].imshow(ashu, cmap=plt.cm.gray)\n",
        "ax[0].set_title('Overlapping objects')\n",
        "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
        "ax[1].set_title('Distances')\n",
        "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
        "ax[2].set_title('Separated objects')\n",
        "\n",
        "for a in ax:\n",
        "    a.set_axis_off()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CqLsWyaZeons"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.measure import label, regionprops\n",
        "from skimage.segmentation import watershed\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# ashu_watershed = watershed(ashu, markers = ashu, connectivity = 1)\n",
        "# plt.imshow(ashu_watershed)\n",
        "sample_l = label(labels)\n",
        "sample_rp = regionprops(sample_l)\n",
        "print('How many Blobs detected?:', len(sample_rp))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "ax.imshow(image, cmap = 'gray')\n",
        "\n",
        "for region in regionprops(sample_l):\n",
        "    # take regions with large enough areas\n",
        "    # if region.area >= 100:\n",
        "        # draw rectangle around segmented coins\n",
        "      minr, minc, maxr, maxc = region.bbox\n",
        "      rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
        "                                  fill=False, edgecolor='red', linewidth=0.5)\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "ax.set_axis_off()"
      ],
      "metadata": {
        "id": "N8UMh0rdeABb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 1, figsize=(12, 12))\n",
        "plt.imshow(labels, cmap = plt.cm.nipy_spectral)"
      ],
      "metadata": {
        "id": "d81iFEV8i29X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Applying threshold\n",
        "threshold = cv2.threshold(ashu, 0, 255,\n",
        "cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "plt.imshow(ashu)\n",
        "\n",
        "from skimage import io, exposure, img_as_ubyte\n",
        "# Convert image to uint8 format from float64\n",
        "img_8bit = img_as_ubyte(exposure.rescale_intensity(threshold))\n",
        "\n",
        "params = cv2.SimpleBlobDetector_Params()\n",
        "\n",
        "detector = cv2.SimpleBlobDetector_create(params)\n",
        "keypoints = detector.detect(img_8bit)\n",
        "\n",
        "blank = np.zeros((1, 1))\n",
        "blobs = cv2.drawKeypoints(img_8bit, keypoints, blank, (0, 0, 255),cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# cv2_imshow(blobs)\n",
        "fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
        "plt.imshow(img_8bit, cmap = 'gray')\n",
        "plt.imshow(blobs, cmap = 'jet')\n"
      ],
      "metadata": {
        "id": "8hSpJRjdX6D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(keypoints)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prpyNrMPaylE",
        "outputId": "ea69743c-745a-4b4e-ffb5-a0f9e11ea38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.feature import peak_local_max"
      ],
      "metadata": {
        "id": "hcQKaNKWOv36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an initial image with two overlapping circles\n",
        "x, y = np.indices((80, 80))\n",
        "x1, y1, x2, y2 = 28, 28, 44, 52\n",
        "r1, r2 = 16, 20\n",
        "mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n",
        "mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n",
        "image = np.logical_or(mask_circle1, mask_circle2)\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBrbU8TwWUSO",
        "outputId": "ab5f3864-fa04-4c8d-f1bb-067930bdb2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to separate the two objects in image\n",
        "# Generate the markers as local maxima of the distance to the background\n",
        "distance = ndi.distance_transform_edt(image)\n",
        "coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)\n",
        "mask = np.zeros(distance.shape, dtype=bool)\n",
        "mask[tuple(coords.T)] = True\n",
        "markers, _ = ndi.label(mask)\n",
        "labels = watershed(-distance, markers, mask=image)"
      ],
      "metadata": {
        "id": "BHIi3Hj8Wl1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(ncols=3, figsize=(9, 3), sharex=True, sharey=True)\n",
        "ax = axes.ravel()\n",
        "\n",
        "ax[0].imshow(image, cmap=plt.cm.gray)\n",
        "ax[0].set_title('Overlapping objects')\n",
        "ax[1].imshow(-distance, cmap=plt.cm.gray)\n",
        "ax[1].set_title('Distances')\n",
        "ax[2].imshow(labels, cmap=plt.cm.nipy_spectral)\n",
        "ax[2].set_title('Separated objects')\n",
        "\n",
        "for a in ax:\n",
        "    a.set_axis_off()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FZs5p8UXWaIT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}